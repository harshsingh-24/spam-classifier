{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Details: -\n",
    "\n",
    "Project Title: - Spam Classifier\n",
    "Developer - Harsh Singh Jadon \n",
    "College - Indian Institute of Technology, Bhubaneswar\n",
    "Date of Completion - 31 July 2021\n",
    "Dataset used - Apache Spam Assassin Dataset\n",
    "Reference - https://www.researchgate.net/publication/310498804_A_study_of_machine_learning_classifiers_for_spam_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the dependencies\n",
    "\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as  mpl\n",
    "import os\n",
    "import math\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\intel'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D:/B.Tech/Github/spam-classfier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\B.Tech\\\\Github\\\\spam-classfier'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "from urllib.request import urlretrieve\n",
    "import shutil\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\" #Link of website\n",
    "\n",
    "HAM_URL_1 = DOWNLOAD_ROOT + \"20030228_easy_ham_2.tar.bz2\" # Ham - Filenames\n",
    "\n",
    "HAM_URL_2 = DOWNLOAD_ROOT + \"20030228_hard_ham.tar.bz2\" \n",
    "\n",
    "HAM_URL_3 = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "\n",
    "SPAM_URL_1 = DOWNLOAD_ROOT + \"20050311_spam_2.tar.bz2\" # Spam - Filename\n",
    "\n",
    "SPAM_URL_2 = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\" \n",
    "\n",
    "PATH = os.path.join(\"dataset\", \"tarfiles\")\n",
    "\n",
    "# Inside the directory we have two tarfiles, named ham.tar.bz2 and spam.tar.bz2\n",
    "def fetch_data(url, path = PATH):\n",
    "    \n",
    "    # In if the directory is not present at the path, make a directory at that location\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    # Filname is the last string present after last '/'\n",
    "    filename = url.rsplit('/', 1)[-1]\n",
    "    # The location where tarfiles will be downloaded\n",
    "    finalpath = os.path.join(path, filename)\n",
    "    \n",
    "    # If file is not present at the location, then get the file from the link\n",
    "    try:\n",
    "        tarfile.open(finalpath)\n",
    "    except:\n",
    "        urlretrieve(url, finalpath)\n",
    "    \n",
    "    # Standard process of extracting data out of a tarfile in a new directory\n",
    "    with tarfile.open(finalpath) as tar:\n",
    "        \n",
    "        # New Directory is created\n",
    "        dirname = os.path.join(tar.getnames()[0])\n",
    "        \n",
    "        # Since I had to run this program a several number of times, I don't want to download cloned copies of the \n",
    "        # same files in the same directory. Therefore, in order to get rid of the problem, I delete the directory\n",
    "        # if already present inside my folder.\n",
    "        if os.path.isdir(dirname):\n",
    "            shutil.rmtree(dirname)\n",
    "            \n",
    "        tar.extractall(path = \"dataset\")\n",
    "        \n",
    "        # In every directory, there is a file named \"cmds\", which we don't want to use as it is a garbage file. \n",
    "        # Therfore we remove it.\n",
    "        \n",
    "        cmds_path = os.path.join(dirname, 'cmds')\n",
    "        \n",
    "        if os.path.isfile(cmds_path):\n",
    "            os.remove(cmds_path)\n",
    "    \n",
    "    return dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_dir_1 = fetch_data(SPAM_URL_1)\n",
    "spam_dir_2 = fetch_data(SPAM_URL_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_dir_1 = fetch_data(HAM_URL_1)\n",
    "ham_dir_2 = fetch_data(HAM_URL_2)\n",
    "ham_dir_3 = fetch_data(HAM_URL_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"D:/B.Tech/Github/spam-classfier/dataset\"\n",
    "HAM_DIR_1 = os.path.join(ROOT, \"easy_ham_2\")\n",
    "HAM_DIR_2 = os.path.join(ROOT, \"hard_ham\")\n",
    "HAM_DIR_3 = os.path.join(ROOT, \"easy_ham\")\n",
    "SPAM_DIR_1 = os.path.join(ROOT, \"spam_2\")\n",
    "SPAM_DIR_2 = os.path.join(ROOT, \"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All filenames are very big in length (near around 40). Avoid any other garbage file if present.\n",
    "\n",
    "ham_filenames_1 = [name for name in sorted(os.listdir(HAM_DIR_1)) if len(name) > 20]\n",
    "ham_filenames_2 = [name for name in sorted(os.listdir(HAM_DIR_2)) if len(name) > 20]\n",
    "ham_filenames_3 = [name for name in sorted(os.listdir(HAM_DIR_3)) if len(name) > 20]\n",
    "spam_filenames_1 = [name for name in sorted(os.listdir(SPAM_DIR_1)) if len(name) > 20]\n",
    "spam_filenames_2 = [name for name in sorted(os.listdir(SPAM_DIR_2)) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 250, 2500, 1397, 500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ham_filenames_1), len(ham_filenames_2), len(ham_filenames_3), len(spam_filenames_1), len(spam_filenames_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy\n",
    "\n",
    "def load_email(is_spam_1, is_spam_2, is_ham_1, is_ham_2, is_ham_3, filename, spam_path = ROOT):\n",
    "    if is_spam_1:\n",
    "        directory = \"spam_2\"\n",
    "    elif is_spam_2:\n",
    "        directory = \"spam\"\n",
    "    elif is_ham_1: \n",
    "        directory = \"easy_ham_2\"\n",
    "    elif is_ham_2:\n",
    "        directory = \"hard_ham\"\n",
    "    else:\n",
    "        directory = \"easy_ham\" \n",
    "    \n",
    "    files = []  # list of all the files\n",
    "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:   # open file in binary format\n",
    "        byte_content = f.read()                                           # read the file\n",
    "        str_content = byte_content.decode('utf-8', errors='ignore')       # Convert the file into UTF-8 format\n",
    "        files.append(str_content)                                         # Adding the UTF-8 format strong to our list\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails_1 = [load_email(is_spam_1=False, is_spam_2=False, is_ham_1=True, is_ham_2=False, is_ham_3=False, filename=name) for name in ham_filenames_1]\n",
    "ham_emails_2 = [load_email(is_spam_1=False, is_spam_2=False, is_ham_1=False, is_ham_2=True, is_ham_3=False, filename=name) for name in ham_filenames_2]\n",
    "ham_emails_3 = [load_email(is_spam_1=False, is_spam_2=False, is_ham_1=False, is_ham_2=False, is_ham_3=True, filename=name) for name in ham_filenames_3]\n",
    "\n",
    "spam_emails_1 = [load_email(is_spam_1=True, is_spam_2=False, is_ham_1=False, is_ham_2=False, is_ham_3=False, filename=name) for name in spam_filenames_1]\n",
    "spam_emails_2 = [load_email(is_spam_1=False, is_spam_2=True, is_ham_1=False, is_ham_2=False, is_ham_3=False, filename=name) for name in spam_filenames_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Return-Path: <exmh-workers-admin@spamassassin.taint.org>\\nDelivered-To: yyyy@localhost.netnoteinc.com\\nReceived: from localhost (localhost [127.0.0.1])\\n\\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id 7106643C34\\n\\tfor <jm@localhost>; Wed, 21 Aug 2002 08:33:03 -0400 (EDT)\\nReceived: from phobos [127.0.0.1]\\n\\tby localhost with IMAP (fetchmail-5.9.0)\\n\\tfor jm@localhost (single-drop); Wed, 21 Aug 2002 13:33:03 +0100 (IST)\\nReceived: from listman.spamassassin.taint.org (listman.spamassassin.taint.org [66.187.233.211]) by\\n    dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g7LCXvZ24654 for\\n    <jm-exmh@jmason.org>; Wed, 21 Aug 2002 13:33:57 +0100\\nReceived: from listman.spamassassin.taint.org (localhost.localdomain [127.0.0.1]) by\\n    listman.redhat.com (Postfix) with ESMTP id F12A13EA25; Wed, 21 Aug 2002\\n    08:34:00 -0400 (EDT)\\nDelivered-To: exmh-workers@listman.spamassassin.taint.org\\nReceived: from int-mx1.corp.spamassassin.taint.org (int-mx1.corp.spamassassin.taint.org\\n    [172.16.52.254]) by listman.redhat.com (Postfix) with ESMTP id 750D33F945\\n    for <exmh-workers@listman.redhat.com>; Wed, 21 Aug 2002 08:30:55 -0400\\n    (EDT)\\nReceived: (from mail@localhost) by int-mx1.corp.spamassassin.taint.org (8.11.6/8.11.6)\\n    id g7LCUqx17585 for exmh-workers@listman.redhat.com; Wed, 21 Aug 2002\\n    08:30:52 -0400\\nReceived: from mx1.spamassassin.taint.org (mx1.spamassassin.taint.org [172.16.48.31]) by\\n    int-mx1.corp.redhat.com (8.11.6/8.11.6) with SMTP id g7LCUqY17578 for\\n    <exmh-workers@redhat.com>; Wed, 21 Aug 2002 08:30:52 -0400\\nReceived: from ratree.psu.ac.th ([202.28.97.6]) by mx1.spamassassin.taint.org\\n    (8.11.6/8.11.6) with SMTP id g7LCGNl23207 for <exmh-workers@redhat.com>;\\n    Wed, 21 Aug 2002 08:16:24 -0400\\nReceived: from delta.cs.mu.OZ.AU (delta.coe.psu.ac.th [172.30.0.98]) by\\n    ratree.psu.ac.th (8.11.6/8.11.6) with ESMTP id g7LCUIl27286;\\n    Wed, 21 Aug 2002 19:30:19 +0700 (ICT)\\nReceived: from munnari.OZ.AU (localhost [127.0.0.1]) by delta.cs.mu.OZ.AU\\n    (8.11.6/8.11.6) with ESMTP id g7LCU1W09629; Wed, 21 Aug 2002 19:30:01\\n    +0700 (ICT)\\nFrom: Robert Elz <kre@munnari.OZ.AU>\\nTo: Chris Garrigues <cwg-dated-1030314468.7c7c85@DeepEddy.Com>\\nCc: exmh-workers@spamassassin.taint.org\\nSubject: Re: New Sequences Window\\nIn-Reply-To: <1029882468.3116.TMDA@deepeddy.vircio.com>\\nReferences: <1029882468.3116.TMDA@deepeddy.vircio.com>\\nMIME-Version: 1.0\\nContent-Type: text/plain; charset=us-ascii\\nMessage-Id: <9627.1029933001@munnari.OZ.AU>\\nX-Loop: exmh-workers@spamassassin.taint.org\\nSender: exmh-workers-admin@spamassassin.taint.org\\nErrors-To: exmh-workers-admin@spamassassin.taint.org\\nX-Beenthere: exmh-workers@spamassassin.taint.org\\nX-Mailman-Version: 2.0.1\\nPrecedence: bulk\\nList-Help: <mailto:exmh-workers-request@spamassassin.taint.org?subject=help>\\nList-Post: <mailto:exmh-workers@spamassassin.taint.org>\\nList-Subscribe: <https://listman.spamassassin.taint.org/mailman/listinfo/exmh-workers>,\\n    <mailto:exmh-workers-request@redhat.com?subject=subscribe>\\nList-Id: Discussion list for EXMH developers <exmh-workers.spamassassin.taint.org>\\nList-Unsubscribe: <https://listman.spamassassin.taint.org/mailman/listinfo/exmh-workers>,\\n    <mailto:exmh-workers-request@redhat.com?subject=unsubscribe>\\nList-Archive: <https://listman.spamassassin.taint.org/mailman/private/exmh-workers/>\\nDate: Wed, 21 Aug 2002 19:30:01 +0700\\n\\n    Date:        Tue, 20 Aug 2002 17:27:47 -0500\\n    From:        Chris Garrigues <cwg-exmh@DeepEddy.Com>\\n    Message-ID:  <1029882468.3116.TMDA@deepeddy.vircio.com>\\n\\n\\n  | I\\'m hoping that all people with no additional sequences will notice are\\n  | purely cosmetic changes.\\n\\nWell, first, when exmh (the latest one with your changes) starts, I get...\\n\\ncan\\'t read \"flist(totalcount,unseen)\": no such element in array\\n    while executing\\n\"if {$flist(totalcount,$mhProfile(unseen-sequence)) > 0} {\\n\\tFlagInner spool iconspool labelup\\n    } else {\\n\\tFlagInner down icondown labeldown\\n    }\"\\n    (procedure \"Flag_MsgSeen\" line 3)\\n    invoked from within\\n\"Flag_MsgSeen\"\\n    (procedure \"MsgSeen\" line 8)\\n    invoked from within\\n\"MsgSeen $msgid\"\\n    (procedure \"MsgShow\" line 12)\\n    invoked from within\\n\"MsgShow $msgid\"\\n    (procedure \"MsgChange\" line 17)\\n    invoked from within\\n\"MsgChange 4862 show\"\\n    invoked from within\\n\"time [list MsgChange $msgid $show\"\\n    (procedure \"Msg_Change\" line 3)\\n    invoked from within\\n\"Msg_Change $msg(id) $show\"\\n    (procedure \"Msg_Show\" line 7)\\n    invoked from within\\n\"Msg_Show cur\"\\n    (\"eval\" body line 1)\\n    invoked from within\\n\"eval $msgShowProc\"\\n    (procedure \"FolderChange\" line 55)\\n    invoked from within\\n\"FolderChange inbox {Msg_Show cur}\"\\n    invoked from within\\n\"time [list  FolderChange $folder $msgShowProc\"\\n    (procedure \"Folder_Change\" line 3)\\n    invoked from within\\n\"Folder_Change $exmh(folder)\"\\n    (procedure \"Exmh\" line 101)\\n    invoked from within\\n\"Exmh\"\\n    (\"after\" script)\\n\\nwhich is probably related to my not having an \"unseen\" sequence anywhere\\n(certainly not in inbox) - I read all of my outstanding mail before I\\ntried this new exmh ...\\n\\nSecond, I\\'ve been used to having a key binding which was to Msg_MarkUnseen\\nwhich doesn\\'t seem to exist any more, and I\\'m not sure what I should replace\\nthat with.   There\\'s obviously a way as the \"Sequences\" menu does this.\\nThe \"Mark Unseen\" menu entry in the message \"More\" menu is still wanting\\nthat function as well...\\n\\n  | For those who have other sequences defined, the window will widen to\\n  | display the other sequences.\\n\\nAny chance of having that lengthen instead?   I like all my exmh stuff\\nin nice columns (fits the display better).   That is, I use the detached\\nfolder list, one column.   The main exmh window takes up full screen,\\ntop to bottom, but less than half the width, etc...\\n\\nI have space for more sequences, in the \"unseen\" window, as long as they\\nremain once nice narrow window (best would be if the sequences could be\\nordered by some preference, then ones which didn\\'t fit would just fall\\noff the bottom, and not be shown).\\n\\nI\\'d also prefer it if that window had no unusual background colouring,\\njust one constant colour - I have been running the unseen window with\\nbackground black, on a root window that is all black, with no borders or\\nother decorations, but made \"sticky\" - the appearance is just like the\\nfolders with unseen messages (and their counts) are written into the\\nroot window (because it is sticky, this small display follows me around\\nand do I can see when new mail needs processing).\\n\\nI also find that I tend to have a bunch of sequences that only ever occur\\nin one folder (some I had forgotten I ever created).  So in addition to\\nthe \"sequences to always show\" and \"sequences to never show\", a\\npreference to only show sequences that occur in more than one folder\\nwould be useful, and then have the sequences that occor only in the\\nfolder I\\'m visiting appear in the list when that folder is current.\\nThis is just to keep the list size somewhat manageable while remaining\\nproductive (I quite often use a sequence to remember a particular message\\nin a folder - the name is used only there, and only for one message,\\nit gives me a handle on the message which remains as the folder is\\npacked, sorted, etc).\\n\\nI haven\\'t updated my exmh for some time now, so I\\'m not sure if this\\nnext one is new, or just new since 2.5, but the Sequences menu (on the\\nbar with New Flist Search ...) only contains \"unseen\" and \"urgent\".\\nIt would be useful if it contained all of the sequences that the folder\\nhappens to have defined.   A \"New sequence\" entry would also be useful\\n(to mark the message with a sequence name that didn\\'t previously exist,\\nwhich can be done now using \"Search\" and the pick interface, but is\\nclumsy that way)\\n\\nActually, you once could, now when I try this, entering a sequence name\\nin the pick box, and a single message number, or a range N-N in the\\nlist of messages, and no pick attributes at all, I now get ...\\n\\nsyntax error in expression \"int(1+1+(1 hit-1)*(3868-1-2)/(4878-1))\"\\n    while executing\\n\"expr int($minlineno+1+($msgid-$minmsgid)*($maxlineno-$minlineno-2)/($maxmsgid-$minmsgid))\"\\n    (procedure \"Ftoc_FindMsg\" line 46)\\n    invoked from within\\n\"Ftoc_FindMsg $msg\"\\n    (procedure \"Ftoc_FindMsgs\" line 5)\\n    invoked from within\\n\"Ftoc_FindMsgs $msgids\"\\n    (procedure \"Ftoc_PickMsgs\" line 5)\\n    invoked from within\\n\"Ftoc_PickMsgs $pick(ids) $pick(addtosel)\"\\n    (procedure \"PickInner\" line 13)\\n    invoked from within\\n\"PickInner {exec pick +inbox -list} {4852 -sequence mercury}\"\\n    (\"uplevel\" body line 1)\\n    invoked from within\\n\"uplevel #0 $cmd\"\\n    (procedure \"busyCursorInner\" line 8)\\n    invoked from within\\n\"busyCursorInner $cmd $widgets\"\\n    (procedure \"busyCursorHack\" line 32)\\n    invoked from within\\n\"busyCursorHack $args\"\\n    (\"cursor\" arm line 1)\\n    invoked from within\\n\"switch $busy(style) {\\n\\ticon\\t\\t{busyIcon $args}\\n\\tcursorAll\\t{busyCursor $args}\\n\\tcursor\\t\\t{busyCursorHack $args}\\n\\tdefault\\t\\t{eval $args}\\n    }\"\\n    (procedure \"busy\" line 3)\\n    invoked from within\\n\"busy PickInner $cmd $msgs\"\\n    (procedure \"Pick_It\" line 51)\\n    invoked from within\\n\"Pick_It\"\\n    invoked from within\\n\".pick.but.pick invoke\"\\n    (\"uplevel\" body line 1)\\n    invoked from within\\n\"uplevel #0 [list $w invoke]\"\\n    (procedure \"tkButtonUp\" line 7)\\n    invoked from within\\n\"tkButtonUp .pick.but.pick\\n\"\\n    (command bound to event)\\n\\nIt has been ages since I did this last though.   I tried adding a Subject\\nto pick on (easy as I know what\\'s in the message...) which made no difference.\\nLooks as if something is now saying \"1 hit\" when before it didn\\'t, or\\nsimilar.\\n\\n  | I\\'ve also changed the ftoc colorization as discussed briefly on the list a \\n  | week or so ago.\\n\\nAny chance of making the current message a little brighter background?\\nJust to make it stand out a fraction more than it does (maybe this is\\nmore apparent to me than many, as I use very small fonts everywhere,\\nthe background of the ftoc line isn\\'t very wide).\\n\\nHope this helps.\\n\\nkre\\n\\n\\n\\n_______________________________________________\\nExmh-workers mailing list\\nExmh-workers@redhat.com\\nhttps://listman.redhat.com/mailman/listinfo/exmh-workers\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "# Demo examples of emails\n",
    "\n",
    "print(ham_emails_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = spam_emails_1 + spam_emails_2 + ham_emails_1 + ham_emails_2 + ham_emails_3\n",
    "y = np.concatenate((np.ones(len(spam_emails_1) + len(spam_emails_2)), np.zeros(len(ham_emails_1) + len(ham_emails_2) + len(ham_emails_3))))\n",
    "\n",
    "# shuffle the dataset at fixed seed\n",
    "X, y = sklearn.utils.shuffle(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some classification problems do not have a balanced number of examples for each class label. \n",
    "# As such, it is desirable to split the dataset into train and test sets in a way that preserves the same \n",
    "# proportions of examples in each class as observed in the original dataset.\n",
    "\n",
    "# This is known as stratified train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4837 4837\n",
      "1210 1210\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(y_train)) # training\n",
    "print(len(X_test), len(y_test)) # testing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Data Preparation using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This technique has been given in Hands on Tensorflow and Keras by O'Reilly\n",
    "\n",
    "import re \n",
    "# Python Regular Expression (Regex)\n",
    "\n",
    "# Boolean function to check whether a string is a url or not\n",
    "def regex_url(s):\n",
    "    url = re.match(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|\"\n",
    "            \"[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-zA-Z]))+\", s)\n",
    "    return url is not None\n",
    "\n",
    "# Per word from string \"words\" we will check first that it a url or not and then replace it with string \"URL\"\n",
    "def url_to_word(words):\n",
    "    for i, word in enumerate(words):\n",
    "        if regex_url(word):\n",
    "            words[i] = \"URL\"\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(email):\n",
    "    \n",
    "    temp_email = \"\"\n",
    "    \n",
    "    for c in email:\n",
    "        if c.isalnum() or c.isspace(): # Only space and alpha-numeric characters are included. \n",
    "            temp_email += c\n",
    "            \n",
    "    return temp_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# This means take that portion of the email, before which two enters have been encountered\n",
    "def remove_header(email):\n",
    "    return email[0][email[0].find('\\n\\n') : ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_word(words):\n",
    "    # Convert all numbers to word \"NUM\"\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        \n",
    "        if word.isdigit():\n",
    "            words[i] = 'NUM'\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer using sklearn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class Email_Features(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, url_to_word=True, to_lowercase=True, num_to_word=True, no_header=True, remove_punc=True):\n",
    "        \n",
    "        self.url_to_word = url_to_word\n",
    "        self.to_lowercase = to_lowercase\n",
    "        self.num_to_word = num_to_word\n",
    "        self.no_header = no_header\n",
    "        self.remove_punc = remove_punc\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        \n",
    "        X_final = []\n",
    "        \n",
    "        for email in X:\n",
    "            \n",
    "            if self.no_header:\n",
    "                email = remove_header(email)\n",
    "            \n",
    "            if self.to_lowercase:\n",
    "                email = email.lower()\n",
    "            \n",
    "            email_words = email.split() # Split by spaces\n",
    "            \n",
    "            if self.url_to_word:\n",
    "                email_words = url_to_word(email_words)\n",
    "            \n",
    "            if self.num_to_word:\n",
    "                email_words = num_to_word(email_words)\n",
    "            \n",
    "            email = ' '.join(email_words) # Concatenate the splitted string, seperated by ' '\n",
    "            \n",
    "            if self.remove_punc:\n",
    "                email = remove_punctuation(email)\n",
    "            \n",
    "            X_final.append(email)\n",
    "            \n",
    "        return X_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nExample of Count Vectoriser: \\ncorpus = [\\n...     'This is the first document.',\\n...     'This document is the second document.',\\n...     'And this is the third one.',\\n...     'Is this the first document?',\\n... ]\\n\\n>>> vectorizer = CountVectorizer()\\n\\n>>> X = vectorizer.fit_transform(corpus)\\n\\n>>> print(vectorizer.get_feature_names())\\n['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\\n\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Convert a collection of text documents to a matrix of token counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Full Pipeline of Data Preparation\n",
    "prepare_pipeline = Pipeline([\n",
    "    ('feature_extraction', Email_Features()),\n",
    "    ('word_set', CountVectorizer())  # vector of count of all the words in the text\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "Example of Count Vectoriser: \n",
    "corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    "\n",
    ">>> vectorizer = CountVectorizer()\n",
    "\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    "\n",
    ">>> print(vectorizer.get_feature_names())\n",
    "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = prepare_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = prepare_pipeline.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4837, 125925)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1210, 49023)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_final.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Classifiers: \n",
    "Multilayer Perceptron(MLP classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# We are going to evaluate our model in terms of precision, accuracy, f1_score and recall metrics.\n",
    "def evaluate(y_predicted, y_theoretical = y_train):\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_theoretical, y_predicted)\n",
    "    \n",
    "    precision = precision_score(y_theoretical, y_predicted)\n",
    "    \n",
    "    recall = recall_score(y_theoretical, y_predicted)\n",
    "    \n",
    "    f1 = f1_score(y_theoretical, y_predicted)\n",
    "    \n",
    "    return {'precision': precision, 'recall': recall, 'f1_score': f1, 'c_matrix': conf_matrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Number of neurons in layer\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes = (16,))\n",
    "\n",
    "classifiers = {\n",
    "    'MLP': mlp_clf,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions per model\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_predictions = {}\n",
    "for classifier_name, classifier in classifiers.items():\n",
    "    # Dictionary of predictions made my each classifier\n",
    "    y_predictions[classifier_name] = cross_val_predict(classifier, X_train_final, y_train, cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Used: MLP\n",
      "Accuracy Score:  0.9797395079594791 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# evaluate each classifier's accuracy\n",
    "for classifier_name, y_predicted in y_predictions.items():\n",
    "    print(\"Classifier Used: {}\".format(classifier_name))\n",
    "    print(\"Accuracy Score: \", accuracy_score(y_train, y_predicted), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier : MLP\n",
      "Precision: 0.9816700610997964\n",
      "Recall: 0.9531970995385629\n",
      "f1-score: 0.9672240802675586\n",
      "Confusion Matrix: \n",
      " [[3293   27]\n",
      " [  71 1446]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of Classifier on the basis of Confusion Matrix, Precision,\n",
    "# recall and f1_score\n",
    "\n",
    "for classifier_name, y_predicted in y_predictions.items():\n",
    "    \n",
    "    metrics = evaluate(y_predicted)\n",
    "    \n",
    "    print(\"Classifier : {}\".format(classifier_name))\n",
    "    \n",
    "    print(\"Precision: {}\".format(metrics['precision']))\n",
    "    \n",
    "    print(\"Recall: {}\".format(metrics['recall']))\n",
    "    \n",
    "    print(\"f1-score: {}\".format(metrics['f1_score']))\n",
    "    \n",
    "    print(\"Confusion Matrix: \\n {}\".format(metrics['c_matrix']), \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate scores for each model\n",
    "scores = {}\n",
    "\n",
    "for classifier_name, classifier in classifiers.items():\n",
    "    method = 'predict_proba'\n",
    "\n",
    "    if not hasattr(classifier, 'predict_proba') and hasattr(classifier, 'decision_function'):\n",
    "        method = 'decision_function'\n",
    "        \n",
    "    scores[classifier_name] = cross_val_predict(classifier, X_train_final, y_train, cv = 3, method = method) #default is 'predict'\n",
    "    \n",
    "    if method == 'predict_proba':\n",
    "        scores[classifier_name] = scores[classifier_name][:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV] activation=relu, hidden_layer_sizes=(16,) .......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........ activation=relu, hidden_layer_sizes=(16,), total= 1.1min\n",
      "[CV] activation=relu, hidden_layer_sizes=(16,) .......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........ activation=relu, hidden_layer_sizes=(16,), total= 1.0min\n",
      "[CV] activation=relu, hidden_layer_sizes=(16,) .......................\n",
      "[CV] ........ activation=relu, hidden_layer_sizes=(16,), total= 1.1min\n",
      "[CV] activation=relu, hidden_layer_sizes=(16,) .......................\n",
      "[CV] ........ activation=relu, hidden_layer_sizes=(16,), total= 1.1min\n",
      "[CV] activation=relu, hidden_layer_sizes=(16,) .......................\n",
      "[CV] ........ activation=relu, hidden_layer_sizes=(16,), total= 1.2min\n",
      "[CV] activation=relu, hidden_layer_sizes=(16, 16) ....................\n",
      "[CV] ..... activation=relu, hidden_layer_sizes=(16, 16), total=  56.9s\n",
      "[CV] activation=relu, hidden_layer_sizes=(16, 16) ....................\n",
      "[CV] ..... activation=relu, hidden_layer_sizes=(16, 16), total= 1.1min\n",
      "[CV] activation=relu, hidden_layer_sizes=(16, 16) ....................\n",
      "[CV] ..... activation=relu, hidden_layer_sizes=(16, 16), total=  56.6s\n",
      "[CV] activation=relu, hidden_layer_sizes=(16, 16) ....................\n",
      "[CV] ..... activation=relu, hidden_layer_sizes=(16, 16), total=  57.9s\n",
      "[CV] activation=relu, hidden_layer_sizes=(16, 16) ....................\n",
      "[CV] ..... activation=relu, hidden_layer_sizes=(16, 16), total=  52.2s\n",
      "[CV] activation=relu, hidden_layer_sizes=(32,) .......................\n",
      "[CV] ........ activation=relu, hidden_layer_sizes=(32,), total= 1.9min\n",
      "[CV] activation=relu, hidden_layer_sizes=(32,) .......................\n",
      "[CV] ........ activation=relu, hidden_layer_sizes=(32,), total= 2.1min\n",
      "[CV] activation=relu, hidden_layer_sizes=(32,) .......................\n",
      "[CV] ........ activation=relu, hidden_layer_sizes=(32,), total= 2.2min\n",
      "[CV] activation=relu, hidden_layer_sizes=(32,) .......................\n",
      "[CV] ........ activation=relu, hidden_layer_sizes=(32,), total= 2.3min\n",
      "[CV] activation=relu, hidden_layer_sizes=(32,) .......................\n",
      "[CV] ........ activation=relu, hidden_layer_sizes=(32,), total= 1.9min\n",
      "[CV] activation=tanh, hidden_layer_sizes=(16,) .......................\n",
      "[CV] ........ activation=tanh, hidden_layer_sizes=(16,), total= 1.1min\n",
      "[CV] activation=tanh, hidden_layer_sizes=(16,) .......................\n",
      "[CV] ........ activation=tanh, hidden_layer_sizes=(16,), total= 1.1min\n",
      "[CV] activation=tanh, hidden_layer_sizes=(16,) .......................\n",
      "[CV] ........ activation=tanh, hidden_layer_sizes=(16,), total= 1.1min\n",
      "[CV] activation=tanh, hidden_layer_sizes=(16,) .......................\n",
      "[CV] ........ activation=tanh, hidden_layer_sizes=(16,), total= 1.0min\n",
      "[CV] activation=tanh, hidden_layer_sizes=(16,) .......................\n",
      "[CV] ........ activation=tanh, hidden_layer_sizes=(16,), total= 1.3min\n",
      "[CV] activation=tanh, hidden_layer_sizes=(16, 16) ....................\n",
      "[CV] ..... activation=tanh, hidden_layer_sizes=(16, 16), total= 1.2min\n",
      "[CV] activation=tanh, hidden_layer_sizes=(16, 16) ....................\n",
      "[CV] ..... activation=tanh, hidden_layer_sizes=(16, 16), total=  56.7s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(16, 16) ....................\n",
      "[CV] ..... activation=tanh, hidden_layer_sizes=(16, 16), total=  55.5s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(16, 16) ....................\n",
      "[CV] ..... activation=tanh, hidden_layer_sizes=(16, 16), total= 1.1min\n",
      "[CV] activation=tanh, hidden_layer_sizes=(16, 16) ....................\n",
      "[CV] ..... activation=tanh, hidden_layer_sizes=(16, 16), total= 1.0min\n",
      "[CV] activation=tanh, hidden_layer_sizes=(32,) .......................\n",
      "[CV] ........ activation=tanh, hidden_layer_sizes=(32,), total= 1.7min\n",
      "[CV] activation=tanh, hidden_layer_sizes=(32,) .......................\n",
      "[CV] ........ activation=tanh, hidden_layer_sizes=(32,), total= 1.7min\n",
      "[CV] activation=tanh, hidden_layer_sizes=(32,) .......................\n",
      "[CV] ........ activation=tanh, hidden_layer_sizes=(32,), total= 1.9min\n",
      "[CV] activation=tanh, hidden_layer_sizes=(32,) .......................\n",
      "[CV] ........ activation=tanh, hidden_layer_sizes=(32,), total= 1.9min\n",
      "[CV] activation=tanh, hidden_layer_sizes=(32,) .......................\n",
      "[CV] ........ activation=tanh, hidden_layer_sizes=(32,), total= 1.8min\n",
      "[CV] activation=logistic, hidden_layer_sizes=(16,) ...................\n",
      "[CV] .... activation=logistic, hidden_layer_sizes=(16,), total= 3.0min\n",
      "[CV] activation=logistic, hidden_layer_sizes=(16,) ...................\n",
      "[CV] .... activation=logistic, hidden_layer_sizes=(16,), total= 2.3min\n",
      "[CV] activation=logistic, hidden_layer_sizes=(16,) ...................\n",
      "[CV] .... activation=logistic, hidden_layer_sizes=(16,), total= 2.6min\n",
      "[CV] activation=logistic, hidden_layer_sizes=(16,) ...................\n",
      "[CV] .... activation=logistic, hidden_layer_sizes=(16,), total= 2.4min\n",
      "[CV] activation=logistic, hidden_layer_sizes=(16,) ...................\n",
      "[CV] .... activation=logistic, hidden_layer_sizes=(16,), total= 2.4min\n",
      "[CV] activation=logistic, hidden_layer_sizes=(16, 16) ................\n",
      "[CV] . activation=logistic, hidden_layer_sizes=(16, 16), total= 2.1min\n",
      "[CV] activation=logistic, hidden_layer_sizes=(16, 16) ................\n",
      "[CV] . activation=logistic, hidden_layer_sizes=(16, 16), total= 2.1min\n",
      "[CV] activation=logistic, hidden_layer_sizes=(16, 16) ................\n",
      "[CV] . activation=logistic, hidden_layer_sizes=(16, 16), total= 2.2min\n",
      "[CV] activation=logistic, hidden_layer_sizes=(16, 16) ................\n",
      "[CV] . activation=logistic, hidden_layer_sizes=(16, 16), total= 2.4min\n",
      "[CV] activation=logistic, hidden_layer_sizes=(16, 16) ................\n",
      "[CV] . activation=logistic, hidden_layer_sizes=(16, 16), total= 2.0min\n",
      "[CV] activation=logistic, hidden_layer_sizes=(32,) ...................\n",
      "[CV] .... activation=logistic, hidden_layer_sizes=(32,), total= 3.5min\n",
      "[CV] activation=logistic, hidden_layer_sizes=(32,) ...................\n",
      "[CV] .... activation=logistic, hidden_layer_sizes=(32,), total= 3.4min\n",
      "[CV] activation=logistic, hidden_layer_sizes=(32,) ...................\n",
      "[CV] .... activation=logistic, hidden_layer_sizes=(32,), total= 3.0min\n",
      "[CV] activation=logistic, hidden_layer_sizes=(32,) ...................\n",
      "[CV] .... activation=logistic, hidden_layer_sizes=(32,), total= 3.0min\n",
      "[CV] activation=logistic, hidden_layer_sizes=(32,) ...................\n",
      "[CV] .... activation=logistic, hidden_layer_sizes=(32,), total= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed: 80.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                     batch_size='auto', beta_1=0.9,\n",
       "                                     beta_2=0.999, early_stopping=False,\n",
       "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
       "                                     learning_rate='constant',\n",
       "                                     learning_rate_init=0.001, max_fun=15000,\n",
       "                                     max_iter=200, momentum=0.9,\n",
       "                                     n_iter_no_change=10,\n",
       "                                     nesterovs_momentum=True, power_t=0.5,\n",
       "                                     random_state=None, shuffle=True,\n",
       "                                     solver='adam', tol=0.0001,\n",
       "                                     validation_fraction=0.1, verbose=False,\n",
       "                                     warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'activation': ['relu', 'tanh', 'logistic'],\n",
       "                         'hidden_layer_sizes': [(16,), (16, 16), (32,)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us make our Neural Network Model reach perfection - Optimization\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# fine-tune MLP classifier\n",
    "mlp_optimised = MLPClassifier()\n",
    "search_space = {\n",
    "    'hidden_layer_sizes': [(16,), (16, 16), (32,)],\n",
    "    'activation': ['relu', 'tanh', 'logistic']\n",
    "}\n",
    "\n",
    "mlp_grid_search = GridSearchCV(mlp_optimised, search_space, 'f1', cv = 5, verbose = 2) #Scoring will be done on the basis of f1_score\n",
    "\n",
    "mlp_grid_search.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_best = mlp_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"spam_classifier_best.sav\"\n",
    "pickle.dump(mlp_best, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi Layer Perceptron Classifier:\n",
      "accuracy: 0.9995865205706016\n",
      "precision: 1.0\n",
      "recall: 0.998681608437706\n",
      "f1-score: 0.99934036939314\n",
      "Confusion Matrix: \n",
      " [[3320    0]\n",
      " [   2 1515]]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = mlp_best.predict(X_train_final)\n",
    "\n",
    "temp = evaluate(y_predicted, y_train)\n",
    "print(\"Multi Layer Perceptron Classifier:\")\n",
    "print(\"accuracy: {}\".format(accuracy_score(y_predicted, y_train)))\n",
    "print(\"precision: {}\".format(temp['precision']))\n",
    "print(\"recall: {}\".format(temp['recall']))\n",
    "print(\"f1-score: {}\".format(temp['f1_score']))\n",
    "print(\"Confusion Matrix: \\n\", temp['c_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(mlp_best, X_train_final, y_train)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of program\n",
    "\n",
    "# Conclusions:  -\n",
    "#     1. With accuracy, precision, recall, f1_score and roc_auc as our estimators, we trained and tested MLP.\n",
    "#     2. Then I optimised it using Grid Search CV and got the best hyper-parameters for my model.\n",
    "#     3. Then I saved my best model for Spam Classifier and predicted using the best hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
